\chapter{DeepInf: Social influence prediction}


\section{Introduction}
\paragraph{} Deepinf focus on the prediction of user-level social influence. It aims to \textbf{predict the action status of a user given the action}.
It is a deep learning based framework to represent both influence dynamics and network structures into a latent space and 
tries to minimize the negative likelihood that was defined in the section 1.

\section{Problem Formulation}
Let $G=(V,E)$ be a static social network, where $V$ denotes the set of users and $E \subseteq V \times V$ denotes the set of undirected relationships. For a user $v$, its \textbf{r-neighbors} are defined as $\Gamma^r_v = \{u: d(u,v) \le r\}$ where $d(u,v)$ is the number of hops (shortest path distance) between $u$ and $v$ in the network $G$. The \textbf{r-ego network} of user $v$ is the subnetwork induced by $\Gamma^r_v$, denoted by $G^r_v$.

Users in social networks perform \textbf{social actions}, such as retweet. At each timestamp $t$, we observe a binary action status of user $u$, $s_u^t \in \{0,1\}$, where $s_u^t=1$ indicates user $u$ has performed this action before or on the timestamp t, and $s_u^t=0$ indicates that the user has not performed this action yet. Such an action log can be available from many social networks.

Given the above definitions, \textbf{social influence locality} can be stated as: users' social decisions and actions are influenced only by their near neighbors within the network, while external sources are assumed to be not present.

\textbf{Problem 1.} \textbf{Social Influence Locality.} Social influence locality models the probability of $v$'s action status conditioned on his r-ego network $G^r_v$ and the action states of her r-neighbors. More formally, given $G^r_v$ and $S^t_v=\{u\in\Gamma_v^r \\ \{v\{\}$, social influence locality aims to quantify the activation probability of $v$ after a given time interval $\Delta t$: $$P\big(s_v^{t+\Delta t} \vert G^r_v, S^t_v\big)$$

We formulate social influence prediction as a binary graph classification problem which can be solved by minimizing the following negative log likelihood objective w.r.t. model parameters $\Theta$: $$L(\Theta) = - \sum_{i=1}^N \log \big( P_\Theta \big( s_{v_i}^{t_i+\Delta t} \vert G_{v_i}^r, S_{v_i}^{t_i}\big)\big)$$

\section{Model framework}
\subsection{Sampling Near Neighbour}

\paragraph{} Give a user $v$, a r-ego network $G_v^r$ is extracted using breadth-first search (BFS) starting from 
user $v$. However, $G_v^r$ may have different size due to the small world property in the social network. Since most
deep learning models expects fixed size data, the graph $G_v^r$ can be sampled to fixed size.
\paragraph{} For sampling a fixed size graph \textbf{random walk the restart} (RWR) was used. RWR algorithm is defined as
following steps.

\begin{itemize}
    \item Start random walks from either the ego user $v$ or one of her active neighbors randomly.
    \item The random walk iteratively travels to its neighborhood with the probability that is 
    proportional to the weight of each edge.
    \item At each step, the walk is assigned a probability to return to the starting node, that is, 
    either the ego user $v$ or one of $v$’s active neighbors.
    \item Run the algorithm until a fixed number of vertices denoted by $\hat{\Gamma_v^r}$ with $\hat{|\Gamma_v^r|}$=n. 
\end{itemize}

\paragraph{} After running this algorithm, a sub-graph $\hat{G_v^r}$ and denote $\hat{S}_v^t$=
\{ $s_u^t :u \in \hat{\Gamma_v^r}$ \} be the action statuses of $v$'s sampled neighbours. Therefore we re-define the optimization
objective in section 1 as:

\begin{equation}
    \mathcal{L}(\theta) = -\sum_{i=1}^N \log(P_{\theta}(s_{v_i}^{t_i+\Delta t}|\hat{\mathcal{G}_{v_i}^{t}},s_{v_i}^{t_i}))
\end{equation}


\subsection{Neural Network}
\paragraph{} With the retreived $\hat{G_{v_i}^{t}}$ and $\hat{S}_v^t$ for each user, an effective
neural network model to incorporate both the structural properties in $\hat{G_{v_i}^{t}}$ and action statuses in 
$\hat{S}_v^t$.

\begin{figure}
    \includegraphics[width=12cm,height=4cm]{tex/img/framework.png}
\end{figure}

\paragraph{} Deepinf neural network model consist of network embedding layer, instance normalization layer, input layer
and GCN layer as described in the above diagram.


\subsubsection{Embedded layer}

\paragraph{} Network embedding technique encode network structural properties into low dimensional matrix $\mathbf{X} \in
\mathbf{R}^{\mathcal{D} \times |V|}$. Deepinf uses Deepwalk algorithm for mapping each users into $\mathbf{R}^{\mathcal{D}}$
space.


\subsubsection{Instance Normalization}

\paragraph{} Instance normalization can remove instance-specific mean and variance, which encourages the downstream model 
to focus on users’ relative positions in latent embedding space rather than their absolute positions. It also prevents 
overfitting.

\paragraph{} Let $x_u$ be the low dimension representation for the user $u \in \hat{\Gamma_v^r}$, the instance normalized
vector $y_u$ is obtained by

\begin{equation}
    y_{ud} = \frac{x_{ud}-\mu_{ud}}{\sqrt{\sigma_d^2+\epsilon}}
\end{equation}

for each embedding dimension d = 1...$\mathcal{D}$, where 
\begin{equation}
    \mu_d = \frac{1}{n}\sum_{u \in \hat{\Gamma_v^r}}x_{ud},\ \sigma_d^2 = \frac{1}{n}\sum_{u \in \hat{\Gamma_v^r}}(x_{ud}-\mu_{ud})^2
\end{equation}


\subsubsection{Input Layer}

\paragraph{} Input later constructs feature vector for each user. Along with the output from the instance Normalization,
input layer adds two binary variable. The first variabe indicates user's action status and second variable indicates whether
the user is the ego user.

\subsubsection{GCN}

\paragraph{} Graph Convolutional Network (GCN) was discussed in detail in the previous chapter. Given the node features 
matrix $\mathcal{H} \in \mathbf{R}^{n \times F}$, for $n$ nodes in the sub-graph, GCN tries to extract neighbourhood 
information for all the nodes in the subgraph and projects into lower dimensional space.

\subsubsection{Output layer and loss function}

\paragraph{} The output layer produces a two dimension representation for each users and comparing the representation for
the ego user with the ground truth value the negative log-likelihood (Eq 6.1) is minimized.

\section{Experimentation}

\subsection{Dataset}

\subsubsection{Digg}
\begin{itemize}
    \item Digg is a \textbf{news aggregator which allows people to vote web content, a.k.a, story, up or down}.
    \item The dataset contains data about stories promoted to Digg’s front page over a period of a month in 2009.
    \item For each story, it contains the list of all Digg users who have voted for the story up to the 
    time of data collection and the time stamp of each vote.
\end{itemize}

\subsubsection{Data preparation}
\begin{itemize}
    \item As dataset is imbalanced (negative instances are more than positive instances), the data is sampled to achieve a balanced distribution.
    \item Each subgraph for the user $v$, has fixed neighbors (50).
\end{itemize}

\subsection{Evaluation metrics}
\paragraph{} The model was trained for 1000 epochs and prediction performance like AUC, Precision, Recall, F1-Score were calculated.
\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| } 
     \hline
     \textbf{Data} & \textbf{Model} & \textbf{AUC} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ 
     \hline
     Digg & DeepInf+GCN (Our's) & 0.82 & 0.58 & 0.634 & 0.604\\ 
     Digg & DeepInf+GCN (Author's) & 0.84 & 0.587 & 0.676 & 0.6288\\ 
     \hline
    \end{tabular}
    \end{center}
\subsubsection{Precision Recall plots}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth,height=4cm]{tex/img/precision_plot.png}
                \caption{Precision plot}
        \end{subfigure}%
        \hfill
    \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth,height=4cm]{tex/img/recall_plot.png}
                \caption{Recall plot}
       \end{subfigure}%
    \end{figure}
\subsubsection{Loss f1-score plots}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{tex/img/loss_plot.png}
                \caption{Loss plot}
        \end{subfigure}%
        \hfill
    \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{tex/img/f1_plot.png}
                \caption{F1-score plot}
       \end{subfigure}%
    \end{figure}


